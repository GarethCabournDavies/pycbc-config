; PyCBC configuration for BNS-NSBH-BBH search on S6 Big Dog data



[workflow]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/initialization.html
file-retention-level = all_triggers


[workflow-datafind]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/datafind.html
datafind-method = AT_RUNTIME_SINGLE_FRAMES

; Look for times when the segment database sayd that data is analyzable, but
; no frame data exists on disk. If any frame data is missing, raise an error
datafind-check-segment-gaps = update_times

; Stat each frame that datafind returns and fail if any frames are missing
datafind-check-frames-exist = raise_error

; Check to see if there are any frames on disk that are not covered in the
; segment_summary table for science segments. This must be "warn" when using
; C00 date, due to known discrepancies between the database segments and GDS
; frames, but should be set to "raise_error" for the final calibration.
datafind-check-segment-summary = warn


[workflow-segments]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/segments.html
segments-method = ALL_SINGLE_IFO_TIME

[workflow-tmpltbank]
; See http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/template_bank.html
tmpltbank-method = PREGENERATED_BANK
tmpltbank-pregenerated-bank = https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/master/S6/pipeline/H1L1-TMPLTBANK-967593543-1209744.xml.gz


[workflow-splittable]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/splittable.html
splittable-method = NOOP
; splittable-num-banks = 20
; splittable-exe-tag = splitbank

[workflow-matchedfilter]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/matched_filter.html
matchedfilter-method = WORKFLOW_INDEPENDENT_IFOS
min-analysis-segments = 11
max-analysis-segments = 11
output-type = hdf

[workflow-coincidence]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/hdf_coincidence.html
background-bins = Low:chirp:3.48 Middle:chirp:6.1 Upper:chirp:12

[workflow-coincidence-full]
parallelization-factor = 10

[workflow-coincidence-inj]
parallelization-factor = 5

[workflow-psd]
parallelization-factor = 10


[llwadd]

[segments_from_cats]

[ligolw_combine_segments]


[splitbank]
; split the template bank up randomly between jobs to ensure equal run time
random-sort =
; set a seed so that the random split is deterministic
random-seed = 666

[inspiral]
; parameters for matched filtering


; amount of buffer data for letting filters settle
pad-data = 8

; conditioning high-pass filter
strain-high-pass = 30

; working sample rate for matched filtering
sample-rate = 4096

; segmentation of the data
; start-pad must be long enough to contain a full BNS signal
segment-length = 256
segment-start-pad = 64
segment-end-pad = 16

; estimation of the noise PSD and construction of the whitening filter
psd-estimation = median
psd-segment-length = 16
psd-segment-stride = 8
psd-inverse-length = 16

; starting frequency of matched filter integration
low-frequency-cutoff = 40

; template approximant
; switch to SEOBNRv2 templates as soon as we can (M >= 4)
approximant = SPAtmplt
order = 7

; threshold for generating triggers
snr-threshold = 5.0

; method for clustering triggers over time
cluster-method = window
cluster-window = 1.0
cluster-function = symmetric

; signal-based vetoes
chisq-bins = 16
newsnr-threshold = 5.0

; options for reducing the computational cost and storage
filter-inj-only = 
injection-window = 2
processing-scheme = mkl


; [workflow-gating]
; see gps_times_er8{a,b}.ini for locations of the pregenerated files
; gating-method = PREGENERATED_FILE

[inspiral-h1]
; Hanford specific matched-filter parameters
channel-name = H1:LDAS-STRAIN

[inspiral-l1]
; Livingston specific matched-filter parameters
channel-name = L1:LDAS-STRAIN

[pegasus_profile-calculate_psd]
condor|request_cpus = 2
; dagman|priority=1000

[calculate_psd]
cores = 2
low-frequency-cutoff = 40
pad-data = 8
strain-high-pass = 30
sample-rate = 4096
segment-length = 256
segment-start-pad = 64
segment-end-pad = 16
psd-estimation = median
psd-segment-length = 16
psd-segment-stride = 8
psd-inverse-length = 16

[merge_psds]

[calculate_psd-h1]
channel-name = H1:LDAS-STRAIN

[calculate_psd-l1]
channel-name = L1:LDAS-STRAIN

[hdf_trigger_merge]

[bank2hdf]

[distribute_background_bins]

[coinc]
; additional time (in seconds) to add to light-travel time to construct time coincidence window
coinc-threshold = 0.005
ranking-statistic = newsnr
strict-coinc-time =

[coinc-full]
; time-slide interval in seconds
timeslide-interval = 0.2
; reduction factor to make lightning bolts for IFAR plots
decimation-factor = 500
; always store the loudest 500 triggers from the time slides
loudest-keep = 1000

[coinc-fullinj&coinc-injfull]
; perform little-dog analysis for software injections
timeslide-interval = 0.2
cluster-window = 10.0
; keep only triggers with a newsnr above 8.5 in injection little-dog analysis
loudest-keep-value = 8.5

[statmap_inj]
ranking-statistic-threshold = 8.5
; time window (in seconds) used to remove triggers around zero-lag coincidences
veto-window = 0.100
; cluster each slide separately over a 10 second window
cluster-window = 10.0

[statmap]
veto-window = 0.100
cluster-window = 10.0


[combine_statmap]
cluster-window = 10.0

[foreground_censor]
strict-coinc-time =

[hdfinjfind]

; time in seconds within which a trigger must fall to be associated with an injection
injection-window = 2.0
optimal-snr-column = H1:alpha1 L1:alpha2
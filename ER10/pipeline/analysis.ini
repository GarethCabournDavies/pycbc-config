; PyCBC configuration for BNS-NSBH-BBH search on O1-ER9-02 data
;
; Documentation for running the workflow generator is here:
;
;    http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/pycbc_make_coinc_search_workflow.html



[workflow]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/initialization.html
; file retention level can take 4 possible values 
; "all_files"    for debugging the pipeline
; "all_triggers" recommended for normal running
; "merged_triggers" used to rerun with file reuse for changes that do not affect
;                single-detector trigger sets but may affect coinc results
; "results"      used to rerun with file reuse to rerun with changes to plots
file-retention-level = all_triggers

[pegasus_profile]
; This section contains default profile information for every job
; This is overriden by profile information set for specific job types
condor|request_memory = 2000
condor|request_cpus = 1

[workflow-datafind]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/datafind.html
datafind-method = AT_RUNTIME_SINGLE_FRAMES

; Look for times when the segment database says that data is analyzable, but
; no frame data exists on disk. If any frame data is missing, raise an error
datafind-check-segment-gaps = raise_error

; Stat each frame that datafind returns and fail if any frames are missing
datafind-check-frames-exist = raise_error

; Check to see if there are any frames on disk that are not covered in the
; segment_summary table for science segments. This must be "warn" when using
; C00 date, due to known discrepancies between the database segments and GDS
; frames, but should be set to "raise_error" for the final calibration.
datafind-check-segment-summary = warn

[workflow-segments]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/segments.html
segments-method = ALL_SINGLE_IFO_TIME

[workflow-tmpltbank]
; See http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/template_bank.html
tmpltbank-method = PREGENERATED_BANK
; NOTE - This bank file may or may not be effectual depending on the data and its PSD!
tmpltbank-pregenerated-bank = https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-config/download/41676894561059629eb5715673d7e6dea7a76865/ER8/bank/H1L1-UBERBANK_MAXM100_NS0p05_ER8HMPSD-1126033217-223200.xml.gz

[workflow-splittable]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/splittable.html
splittable-method = IN_WORKFLOW
splittable-exe-tag = splitbank

[workflow-splittable-full_data]
; empirically tuned on O1 data to give runtimes between 1 and 10 hours
splittable-num-banks = 8

[workflow-splittable-injections]
; empirically tuned on O1 data to give runtimes between a few mins and 10 hours
splittable-num-banks = 2

[workflow-matchedfilter]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/matched_filter.html
matchedfilter-method = WORKFLOW_INDEPENDENT_IFOS
min-analysis-segments = 1
min-analysis-length = 528
max-analysis-segments = 5
output-type = hdf
plot-throughput =

[workflow-coincidence]
; http://ligo-cbc.github.io/pycbc/releases/v1.2.0/html/workflow/hdf_coincidence.html
do-trigger-fitting =
; any cuts to remove super-glitchy templates are assumed to have been made in the bank
background-bins = all:total:lt600

[workflow-coincidence-full]
parallelization-factor = 20

[workflow-coincidence-inj]
parallelization-factor = 5

[workflow-psd]
parallelization-factor = 10

[workflow-gating]
; not clear if this has any meaning now - FIXME
gating-method = PREGENERATED_FILE

[llwadd]

[segments_from_cats]

[ligolw_combine_segments]

[splitbank]
; split the template bank up randomly between jobs to ensure equal run time
random-sort =
; set a seed so that the random split is deterministic
random-seed = 666

[inspiral]
; parameters for matched filtering

; amount of buffer data for letting filters settle
pad-data = 8

; conditioning high-pass filter
strain-high-pass = 20

; working sample rate for matched filtering
sample-rate = 2048

; segmentation of the data
; start-pad must be long enough to contain a full BNS signal
segment-length = 512
segment-start-pad = 112
segment-end-pad = 16
; turn on zero-padding
allow-zero-padding =
; Taper the first and last second of data read in for zero padding
taper-data = 1

; estimation of the noise PSD and construction of the whitening filter
psd-estimation = median
psd-segment-length = 16
psd-segment-stride = 8
psd-inverse-length = 16
; 512s PSD length given by:
; (psd-segment-length + psd-num-segments - 1 * psd-segment-stride)
psd-num-segments = 63

; Autogating options
autogating-threshold = 100
autogating-cluster = 0.5
autogating-width = 0.25
autogating-taper = 0.25
autogating-pad = 16

; starting frequency of matched filter integration
low-frequency-cutoff = 30

; template approximant
; switch to SEOBNRv4 templates as soon as we can (M >= 4)
approximant = 'SPAtmplt:mtotal<4' 'SEOBNRv4_ROM:else'
order = -1

; threshold for generating triggers
snr-threshold = 5.5

; method for clustering triggers over time
cluster-method = window
cluster-window = 1
cluster-function = symmetric

; signal-based vetoes
chisq-bins = "1.75*(get_freq('fSEOBNRv2Peak',params.mass1,params.mass2,params.spin1z,params.spin2z)-60.)**0.5"
newsnr-threshold = 5

; options for reducing the computational cost and storage
filter-inj-only = 
injection-window = 4.5
processing-scheme = mkl

[inspiral-h1]
; Hanford specific matched-filter parameters
channel-name = ${workflow|h1-channel-name}

[inspiral-l1]
; Livingston specific matched-filter parameters
channel-name = ${workflow|l1-channel-name}

[pegasus_profile-calculate_psd]
condor|request_memory = 8000
condor|request_cpus = ${calculate_psd|cores}
dagman|priority = 1000

[calculate_psd]
cores = 4
low-frequency-cutoff = ${inspiral|low-frequency-cutoff}
pad-data = ${inspiral|pad-data}
strain-high-pass = ${inspiral|strain-high-pass}
sample-rate = ${inspiral|sample-rate}
segment-length = ${inspiral|segment-length}
segment-start-pad = ${inspiral|segment-start-pad}
segment-end-pad = ${inspiral|segment-end-pad}
psd-estimation = ${inspiral|psd-estimation}
psd-segment-length = ${inspiral|psd-segment-length}
psd-segment-stride = ${inspiral|psd-segment-stride}
psd-num-segments = ${inspiral|psd-num-segments}
taper-data = ${inspiral|taper-data}
autogating-threshold = ${inspiral|autogating-threshold}
autogating-cluster = ${inspiral|autogating-cluster}
autogating-width = ${inspiral|autogating-width}
autogating-taper = ${inspiral|autogating-taper}
autogating-pad = ${inspiral|autogating-pad}

[merge_psds]

[calculate_psd-h1]
channel-name = ${workflow|h1-channel-name}

[calculate_psd-l1]
channel-name = ${workflow|l1-channel-name}

[pegasus_profile-hdf_trigger_merge]
condor|request_memory = 4000

[hdf_trigger_merge]

[bank2hdf]

[pegasus_profile-fit_by_template]
condor|request_memory = 8000

[fit_by_template]
fit-function = exponential
sngl-stat = new_snr
stat-threshold = 6.
prune-param = mtotal
log-param =
prune-bins = 2
prune-number = 2

[fit_over_param]
fit-param = template_duration
; f_low required to calculate template duration to smooth the fit over
f-lower = 30.
log-param =
regression-method = tricube
smoothing-width = 0.2

[pegasus_profile-distribute_background_bins]
condor|request_memory = 6000

[distribute_background_bins]

[pegasus_profile-coinc]
condor|request_memory = 3000

[coinc]
; additional time (in seconds) to add to light-travel time to construct time coincidence window
coinc-threshold = 0.005
strict-coinc-time =
ranking-statistic = phasetd_exp_fit_stat

; Parameters to reproduce this file available at
; https://code.pycbc.phy.syr.edu/ligo-cbc/pycbc-software/blob/c2fc41b3c32dbaa2fe859b0d5929b050c01c2b91/statistic-files/v1/make_h1l1_phase_time_amp_v1.sh
; change sugwg-condor.phy.syr.edu to code.pycbc.phy.syr.edu once Larne gets a new host certificate
statistic-files = gsiftp://pycbc.phy.syr.edu/var/opt/gitlab/ligo-cbc/pycbc-software/statistic-files/v1/H1L1-PHASE_TIME_AMP_v1.hdf

[coinc-full]
; time-slide interval in seconds
timeslide-interval = 0.1
; reduction factor to make lightning bolts for IFAR plots
decimation-factor = 5000
; always store time slide coincs with a stat above specified value
loudest-keep-value = 8.5

[coinc-fullinj&coinc-injfull]
; perform little-dog analysis for software injections
timeslide-interval = ${coinc-full|timeslide-interval}
cluster-window = ${statmap|cluster-window}
; keep only coincs with a stat above specified value in injection little-dog analysis
loudest-keep-value = 8.5

[pegasus_profile-statmap&pegasus_profile-statmap_inj]
condor|request_memory = 4000

[statmap&statmap_inj]
; time window (in seconds) used to remove triggers around zero-lag coincidences
veto-window = 0.100
; cluster each slide separately over a 10 second window
cluster-window = 10.0

[combine_statmap]
cluster-window = ${statmap|cluster-window}

[foreground_censor]
strict-coinc-time =

[hdfinjfind]
; time in seconds within which a trigger must fall to be associated with an injection
injection-window = 2.0
optimal-snr-column = H1:alpha1 L1:alpha2
